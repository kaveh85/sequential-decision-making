{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de490773",
   "metadata": {},
   "source": [
    "# Lesson 01: Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a69fa7",
   "metadata": {},
   "source": [
    "## ðŸ§  What is an MDP?\n",
    "\n",
    "A **Markov Decision Process (MDP)** is a formal framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker.\n",
    "\n",
    "An MDP is defined by a 5-tuple:\n",
    "\n",
    "$$ (S, A, P, R, \\gamma) $$\n",
    "\n",
    "Where:\n",
    "- **S**: Set of states  \n",
    "- **A**: Set of actions  \n",
    "- **P(s'|s,a)**: Transition probability function â€“ the probability of transitioning to state $s'$ from state $s$ by taking action $a$  \n",
    "- **R(s,a)**: Reward function â€“ the immediate reward received after taking action $a$ in state $s$  \n",
    "- **$\\gamma$**: Discount factor â€“ determines how future rewards are valued ($0 \\leq \\gamma \\leq 1$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe1563",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Objective\n",
    "\n",
    "The goal in an MDP is to find a **policy** $\\pi$, which maps states to actions:\n",
    "\n",
    "$$ \\pi(s) = a $$\n",
    "\n",
    "This policy should maximize the expected **cumulative discounted reward** over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5e784",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Value Functions\n",
    "\n",
    "### State-Value Function:\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right]\n",
    "$$\n",
    "\n",
    "The expected return starting from state $s$ and following policy $\\pi$.\n",
    "\n",
    "### Action-Value Function:\n",
    "$$\n",
    "Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_0 = a \\right]\n",
    "$$\n",
    "\n",
    "The expected return from state $s$, taking action $a$, and then following policy $\\pi$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8e97a",
   "metadata": {},
   "source": [
    "## ðŸ§® Bellman Equations\n",
    "\n",
    "### Bellman Expectation Equation:\n",
    "$$\n",
    "V^\\pi(s) = \\sum_a \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "### Bellman Optimality Equation:\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s') \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac57b5c",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "- MDPs model decision-making in stochastic environments.  \n",
    "- The value function evaluates how good it is to be in a state (or take an action).  \n",
    "- The Bellman equations provide recursive definitions that are the foundation of most solution algorithms.\n",
    "\n",
    "Coming up in Lesson 2: **Bellman Equations in Practice & Deriving Value Functions**\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

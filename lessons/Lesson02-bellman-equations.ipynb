{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80914d6",
   "metadata": {},
   "source": [
    "# Lesson 02: Bellman Equations in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f8d82",
   "metadata": {},
   "source": [
    "## üîÅ Recap from Lesson 1\n",
    "\n",
    "We defined value functions and Bellman equations:\n",
    "\n",
    "- State-value function:\n",
    "  $$\n",
    "  V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s \\right]\n",
    "  $$\n",
    "\n",
    "- Bellman expectation equation:\n",
    "\n",
    "  $$\n",
    "  V^\\pi(s) = \\sum_a \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b59e9",
   "metadata": {},
   "source": [
    "## üß† Value of a Policy\n",
    "\n",
    "Given a fixed policy \\( \\pi \\), we can compute how good it is using **policy evaluation**.\n",
    "\n",
    "This means solving:\n",
    "$$\n",
    "V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V^\\pi(s')\n",
    "$$\n",
    "\n",
    "We can solve this using **iterative updates**:\n",
    "\n",
    "1. Initialize \\( V(s) = 0 \\) for all \\( s \\)\n",
    "2. For each state, apply:\n",
    "   $$\n",
    "   V_{k+1}(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V_k(s')\n",
    "   $$\n",
    "3. Repeat until values converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0faf4e2",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Example MDP\n",
    "\n",
    "Let‚Äôs consider this toy MDP:\n",
    "\n",
    "- States: $s_1$, $s_2$\n",
    "- Actions: $a_1$, $a_2$\n",
    "- Policy: $\\pi(s_1) = a_1$, $\\pi(s_2) = a_2$\n",
    "- Transition:\n",
    "  - $P(s_2 | s_1, a_1) = 1$\n",
    "  - $P(s_1 | s_2, a_2) = 1$\n",
    "- Rewards:\n",
    "  - $R(s_1, a_1) = 2$\n",
    "  - $R(s_2, a_2) = 1$\n",
    "- $\\gamma = 0.9$\n",
    "\n",
    "Starting from $V_0(s_1) = V_0(s_2) = 0$, let's compute $V_1$, $V_2$, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation (Iterative)\n",
    "gamma = 0.9\n",
    "V = {'s1': 0, 's2': 0}\n",
    "rewards = {'s1': 2, 's2': 1}\n",
    "transitions = {'s1': 's2', 's2': 's1'}\n",
    "\n",
    "for k in range(5):  # 5 iterations\n",
    "    new_V = {}\n",
    "    for s in V:\n",
    "        next_s = transitions[s]\n",
    "        new_V[s] = rewards[s] + gamma * V[next_s]\n",
    "    V = new_V\n",
    "    print(f\"Iteration {k+1}: V = {V}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e54edb",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- The Bellman expectation equation allows us to **evaluate** how good a given policy is.\n",
    "- We use **iterative updates** to approximate the value function.\n",
    "- Once we know the value of a policy, we can try improving it (covered in Lesson 3).\n",
    "\n",
    "Up next: **Policy Iteration ‚Äì Evaluating and Improving Policies**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
